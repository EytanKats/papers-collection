<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Kiu-net Towards Accurate Segmentation of Biomedical Images Using Over-complete Representations | fastpages</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Kiu-net Towards Accurate Segmentation of Biomedical Images Using Over-complete Representations" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper summary" />
<meta property="og:description" content="Paper summary" />
<link rel="canonical" href="https://eytankats.github.io/papers-collection/segmentation/medical/ultrasound/histopathology/retinal%20images/miccai/2021/06/14/valanarasu-kiu-net.html" />
<meta property="og:url" content="https://eytankats.github.io/papers-collection/segmentation/medical/ultrasound/histopathology/retinal%20images/miccai/2021/06/14/valanarasu-kiu-net.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-14T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://eytankats.github.io/papers-collection/segmentation/medical/ultrasound/histopathology/retinal%20images/miccai/2021/06/14/valanarasu-kiu-net.html","@type":"BlogPosting","headline":"Kiu-net Towards Accurate Segmentation of Biomedical Images Using Over-complete Representations","dateModified":"2021-06-14T00:00:00-05:00","datePublished":"2021-06-14T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://eytankats.github.io/papers-collection/segmentation/medical/ultrasound/histopathology/retinal%20images/miccai/2021/06/14/valanarasu-kiu-net.html"},"description":"Paper summary","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/papers-collection/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://eytankats.github.io/papers-collection/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/papers-collection/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/papers-collection/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/papers-collection/about/">About Me</a><a class="page-link" href="/papers-collection/search/">Search</a><a class="page-link" href="/papers-collection/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Kiu-net Towards Accurate Segmentation of Biomedical Images Using Over-complete Representations</h1><p class="page-description">Paper summary</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-06-14T00:00:00-05:00" itemprop="datePublished">
        Jun 14, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      1 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/papers-collection/categories/#segmentation">segmentation</a>
        &nbsp;
      
        <a class="category-tags-link" href="/papers-collection/categories/#medical">medical</a>
        &nbsp;
      
        <a class="category-tags-link" href="/papers-collection/categories/#ultrasound">ultrasound</a>
        &nbsp;
      
        <a class="category-tags-link" href="/papers-collection/categories/#histopathology">histopathology</a>
        &nbsp;
      
        <a class="category-tags-link" href="/papers-collection/categories/#retinal images">retinal images</a>
        &nbsp;
      
        <a class="category-tags-link" href="/papers-collection/categories/#miccai">miccai</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Valanarasu, J.M.J., Sindagi, V.A., Hacihaliloglu, I. and Patel, V.M.<br />
In International Conference on Medical Image Computing and Computer-Assisted Intervention 2020</p>

<p><a href="https://arxiv.org/abs/2006.04878">ArXiv</a> <a href="https://github.com/jeya-maria-jose/KiU-Net-pytorch">GitHub</a></p>

<h2 id="main-contribution">Main contribution</h2>
<p>Paper proposes a novel architecture combining the features of both under-complete and over-complete deep networks.<br />
This architecture captures finer details better than the standard encoder-decoder architecture of U-Net
thus aiding in precise segmentation.</p>

<h2 id="intuition">Intuition</h2>
<p>The standard encoder-decoder architecture of U-Net belongs to the family of under-complete convolution auto-encoders.
The increasing receptive field size over the depth of the network constrains it to focus more
on the higher-level features.<br />
However, it is important to note that tiny structures require smaller receptive fields.<br />
In over-complete architectures the data is projected onto a higher dimension in the intermediate layers.
The filters in this type of architecture learn finer low-level features due to the decreasing size
of receptive field as we go deeper in the encoder network.
<img src="/papers-collection/images/2021-06-14-valanarasu-kiu-net/receptive_field.png" alt="" /></p>

<h2 id="technical-details">Technical details</h2>

<h3 id="architecture">Architecture</h3>
<p>Model (KiU-Net) consists of two branches when one of them is an over-complete network (KiNet)
and other - under-complete network (UNet).<br />
Features from two branches combined at each block level by cross residual fusion block (CRFB).
This block extracts complementary features from both network branches and forwards to both of them respectively.<br />
Finally, the features from both branches are added and forwarded through 1x1 convolution layer
to produce the segmentation mask.</p>

<p><img src="/papers-collection/images/2021-06-14-valanarasu-kiu-net/architecture.png" alt="" /></p>

<h2 id="results">Results</h2>

<h3 id="filter-responses">Filter responses</h3>

<p><img src="/papers-collection/images/2021-06-14-valanarasu-kiu-net/filter_responses.png" alt="" /></p>

<h3 id="2d-ultrasound-brain-scans-of-preterm-neonates">2D Ultrasound brain scans of preterm neonates</h3>
<p>The dataset was collected during the study.
Ventricles and septum pellecudi were manually annotated by an expert ultrasonographer.</p>

<p>Qualitative comparison:<br />
<img src="/papers-collection/images/2021-06-14-valanarasu-kiu-net/us_dataset_qualitative_results.png" alt="" /></p>

<p>Quantitative comparison:<br />
<img src="/papers-collection/images/2021-06-14-valanarasu-kiu-net/us_dataset_quantitative_results.png" alt="" /></p>

<h3 id="gland-segmentation-glas-dataset">GLAnd Segmentation (GLAS) dataset</h3>
<p>GLAnd Segmentation (GLAS) dataset contains microscopic images of Hematoxylin and Eosin (H&amp;E) stained slides
and the corresponding ground truth annotations by expert pathologists.</p>

<p>Qualitative comparison:<br />
<img src="/papers-collection/images/2021-06-14-valanarasu-kiu-net/glas_dataset_qualitative_results.png" alt="" /></p>

<p>Quantitative comparison:<br />
<img src="/papers-collection/images/2021-06-14-valanarasu-kiu-net/glas_dataset_quantitative_results.png" alt="" /></p>

<h3 id="retinal-images-vessel-tree-extraction-rite-dataset">Retinal Images vessel Tree Extraction (RITE) dataset:</h3>
<p>RITE dataset contains segmentation of arteries and veins on retinal fundus images.</p>

<p>Qualitative comparison:<br />
<img src="/papers-collection/images/2021-06-14-valanarasu-kiu-net/rite_dataset_qualitative_results.png" alt="" /></p>

<p>Quantitative comparison:<br />
<img src="/papers-collection/images/2021-06-14-valanarasu-kiu-net/rite_dataset_quantitative_results.png" alt="" /></p>

<h3 id="ablation-study">Ablation study</h3>

<p><img src="/papers-collection/images/2021-06-14-valanarasu-kiu-net/ablation_study.png" alt="" /></p>

<p>UC - Under-Complete architecture<br />
OC - Over-Complete architecture<br />
SK - SKip connections</p>


  </div><a class="u-url" href="/papers-collection/segmentation/medical/ultrasound/histopathology/retinal%20images/miccai/2021/06/14/valanarasu-kiu-net.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/papers-collection/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/papers-collection/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/papers-collection/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/papers-collection/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/papers-collection/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
